{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial\n",
    "\n",
    "This notebook incorporates, analyses, experiments with and comments on the neural network and deep learning tutorial contained in [NeuralNetworksAndDeepLearning.com](http://neuralnetworksanddeeplearning.com/chap1.html#exercises_191892)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Loader\n",
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron/Sigmoid Neuron\n",
    "\n",
    "We begin with the input layer consisting of a vectorised set of inputs $x_i$. For instance, in the MNIST example, if each image (of a digit character) is described by a grid of 28 X 28 pixels where the intensity at each pixel is a grayscale value from 0.0 to 1.0, the input is then a 28 X 28 = 784 length vector where each component ranges from 0.0 to 1.0\n",
    "\n",
    "A single *perceptron* neuron consists of a linear combination of inputs $\\sum_j w_j x_j$ and a threshold function that outputs a 1 if the linear combination exceeds the threshold $b$. In other words, in vector form:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{output} &= \\left\\{\n",
    "\\begin{array}{ll}\n",
    "0 \\text{  if  } \\sum_j w_j x_j \\leq -b \\\\\n",
    "1 \\text{  if  }  \\sum_j w_j x_j > -b\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "\n",
    "The sigmoid neuron is an additional nonlinear transformation that is applied to the affine transformations above, where typically it is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(z) & \\equiv \\frac{1}{1 + e\\,^{-z}} \\\\\n",
    "&= \\frac{1}{1 + \\exp\\left(-\\sum_j w_j x_j - b \\right)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "The sigmoid function has the form shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = np.arange(-5,5,0.1)\n",
    "sigma = 1 / (1+np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAE4CAYAAAAHP8f5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZKqgoElSaoKBpYkqSoYWJKkqvD/j7UQmTBk4ikAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106db2c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(z,sigma)\n",
    "plt.plot([0,0], [0.0, 1.0], color='r', linestyle='-', linewidth=2)\n",
    "plt.plot([-6,0], [0.001, 0.001], color='r', linestyle='-', linewidth=2)\n",
    "plt.plot([0,6], [0.99, 0.99], color='r', linestyle='-', linewidth=2)\n",
    "\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer Fundamental Equation\n",
    "The fundamental equation that serves as the building block for the whole network is:\n",
    "\n",
    "\\begin{align}\n",
    "z\\,^l_j & = \\sum_k w\\,^l_{jk} a\\,^{l-1}_k + b\\,^l_j \\\\\n",
    "a\\,^l_j  & = \\sigma \\left( z\\,^l_j \\right)\n",
    "\\end{align}\n",
    "\n",
    "where $l$ is the present layer, $l-1$ is the previous layer, $a\\,^l_j$ is the $j$th component of the vector of *activations* $a\\,^l$ of the $l$th layer, ie the activation of the neuron $j$ in layer $l$.\n",
    "\n",
    "Now, for each layer $l$ we have a weight matrix, $w\\,^l$, where each element $w\\,^l_{jk}$ gives the weight from neuron $k$ in layer $l-1$ to neuron $j$ in the present layer $l$. The bias vector is then given by components $b_j$. Note the order of the indices of the weight matrix $w\\,^l$: the rows stand for the present layer, layer $l$, while the coloumns represent the prior layer, $l-1$. Thus, abusing the nomenclature somewhat, arrows in the network go from coloumns to the rows. More precisely, the weight matrix is a linear transformation of the outputs of the previous layer generating the inputs to the next layer. \n",
    "\n",
    "We can then write the above equation in matrix form as:\n",
    "\n",
    "\\begin{align}\n",
    "a\\,^l  & = \\sigma \\left( w\\,^l a\\,^{l-1} + b\\,^l\\right)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea behind learning\n",
    "\n",
    "We desire a network which learns the optimal weigths and biases so that the correct output is generated for a given input, eg an input of a handwritten image of a digit is correctly classified as the intended digit. \n",
    "\n",
    "The network learns by making small adjustments to the weights thereby causing correspondingly small modifications to the output in such a manner that a defined error is reduced, for instance the classification error. For instance, if the image of a \"9\" is misclassified as an \"8,\" we require the weights and biases to be revised so that a small move is made to the correct output. This is then repeated so as to systematically improve the classification error. \n",
    "\n",
    "The problem with perceptrons is that a small change to the weights can cause a corresponding large change in the output owing to the fact the output of the perceptron, a \"0\" say, is flipped to a \"1\". This flip can in turn result in the behaviour of the rest of the network to alter in unexpected, possibly large, ways. \n",
    "\n",
    "The sigmoid neuron gets around this probem by ensuring that a small change in the output of the neuron results from an alternation of the weights and biases. It is this \"continuity\" of the modification that gives the network sufficient traction so that it can move steadily in the direction of overall improvement or learning. \n",
    "\n",
    "In summary, it is the smoothness of the sigmoidal neuron that is privotal to enabling learning to take place. In other words, small changes $\\Delta w_i$ and $\\Delta b$ in the weights and biases respectively lead to small changes in the output:\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta \\text{output} \\approx \\sum_j \\frac{\\partial \\,output}{\\partial w_j} \\Delta w_j +   \\frac{\\partial \\,output}{\\partial b} \\Delta b\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "We start with a simple quadratic cost funtion defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "C(w, b) \\equiv \\frac{1}{2n} \\sum_x \\left\\lVert y(x) - a^L(x) \\right\\rVert\n",
    "\\end{equation}\n",
    "\n",
    "where the sum is taken over the $n$ training samples. This will be recognised as bascially the mean square error of the output of the network, $a^L(x)$.  The desired output of the network is $y(x)$, defined so that, for an input digit \"6\" say, $y(x) = (0, 0, 0, 0, 0, 1, 0, 0, 0, 0)^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The stochastic gradient descent algorithm\n",
    "\n",
    "The cost function $C(w,b)$ can be minimised with respect to $w$ and $b$ by the standard gradient descent update rule:\n",
    "\n",
    "\\begin{align}\n",
    "w_k \\leftarrow w'_k &= w_k - \\eta \\frac{\\partial C}{\\partial w_k} \\\\ \\\\\n",
    "b_k \\leftarrow b'_k &= b_k - \\eta \\frac{\\partial C}{\\partial b_k} \\\\ & \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We repeatedly apply the above update equations until we reach a certain fixed number of iterations expires or some other termination criterion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition behind Back Propagation\n",
    "\n",
    "In order to apply the stochatic gradient algorithm to obtain the optimal network parameters, we require the partial derivatives $\\partial C \\,/ \\, \\partial w^l_{jk}$ and  $\\partial C \\,/ \\, \\partial b^l_{j}$. This is perfromed in an efficient manner by computing intermediate quantities and then propagating these values backward through the network. \n",
    "\n",
    "The fundamental intermediate quantity is the error term $\\delta^l_j$ associated with the $l$th layer and the $j$th neuron. Recalling the discussion above about the idea of learning, suppose that a small change $\\Delta z^l_j$ is introduced at neuron $j$ in layer $l$. This means that the output of this neuron is now revised to $\\sigma(z^l_j + \\Delta z^l_j)$. This change is then propoagated through all subsequent layers eventually causing the cost to be adjusted by an amount $\\frac{\\partial C}{\\partial z^l_j} \\Delta z^l_j$.\n",
    "\n",
    "Now, suppose that it is possible to exercise a measure of control over the $\\Delta z^l_j$, how would we want to adjust these? Well, since we ultimately wish to minimise the cost, if it turns out that $\\frac{\\partial C}{\\partial z^l_j}$ is either large and positive or large and negative, we should make $\\Delta z^l_j$ correspondingly large with the opposite sign. If, on the other hand, $\\frac{\\partial C}{\\partial z^l_j}$ is near Zero, the given neuron is near optimal, so we should expect only a small change in $\\Delta z^l_j$.\n",
    "\n",
    "This motivates heuristically designating $\\delta^l_j$ as an error term:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j}\n",
    "\\end{equation}\n",
    "\n",
    "### Backpropagation Equations\n",
    "\n",
    "\\begin{align}\n",
    "\\text{(BP1}) \\qquad & \\delta^L = \\nabla_a C \\odot \\sigma'(z^L)  \\\\ \\\\\n",
    "\\text{(BP2}) \\qquad & \\delta^l =  ((w^{l+1})^T \\delta^{l+1} ) \\odot \\sigma'(z^l) \\\\  \\\\\n",
    "\\text{(BP3}) \\qquad & \\frac{\\partial C}{\\partial b^l_j}  = \\delta^l_j \\\\ \\\\\n",
    "\\text{(BP4}) \\qquad & \\frac{\\partial C}{\\partial w^l_{jk}}  = a^{l-1}_k\\delta^l_j \\\\ \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (BP1) Error in the output layer\n",
    "\n",
    "The backpropagation equations exploit the chain rule for derivatives, including this one. So, this equation measures how fast the cost fuction is changing at each of the output activation layers and adjusts this by the rate of change of the activation function with respect to $z^L$, the linear combination of the output activation of the previous layer and bias terms.\n",
    "\n",
    "With a quadratic cost, $\\nabla_a C$ simplifies to $\\nabla_a C = (a^L - y)$. The components of $a^L$ come from the feedforward stage, to be described later. \n",
    "\n",
    "### (BP2) Recursive equation for the error term at any layer\n",
    "\n",
    "This equation gives the error in layer $l$ as a function of layer $l+1$. Since we are back propagating, we suppose the current layer is $l+1$, so what we require is the error in the *next* layer (from the point of view of traversing back along the network). \n",
    "\n",
    "The idea behind this equation is that we are propagating the current error, $\\delta^{l+1}$ back to the next layer $l$. To get a clearer appreciation of what is happening here, we consider the derivation of this term, again an application of the chain rule. \n",
    "\n",
    "\\begin{align}\n",
    "\\delta^l_j & = \\frac{\\partial C}{\\partial z^l_j}\\\\\n",
    "& = \\sum_k \\frac{\\partial C}{\\partial z^{l+1}_k} \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \n",
    "= \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_j}  \\delta^{l+1}_k\\\\\n",
    "\\end{align}\n",
    "\n",
    "Now recall, from the definition of the affine transformation of the outputs of the activation layer, we had:\n",
    "\n",
    "\\begin{align}\n",
    "z\\,^l_j & = \\sum_k w\\,^l_{jk} a\\,^{l-1}_k + b\\,^l_j \n",
    "\\end{align}\n",
    "\n",
    "With a suitable substitution of indices we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "z^{l+1}_k = \\sum_j w^{l+1}_{kj} \\sigma(z^l_j) + b^{l+1}_k \\\\\n",
    "\\end{align}\n",
    "\n",
    "What this gives are the contributions to the output of the linear combiner for layer $l+1$ from each of the outputs of layer $l$, namely $a^l_j = \\sigma(z^l_j)$ multiplied by the parameter from the $j$th neuron in layer $l$ to the $k$th neuron in layer $l+1$.\n",
    "\n",
    "Then, performing the differentiation, we have:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = w^{l+1}_{kj} \\sigma'(z^l_j).\\\\\n",
    "\\end{align}\n",
    "\n",
    "And finally, \n",
    "\n",
    "\\begin{align}\n",
    "\\delta^l_j & = \\sum_k w^{l+1}_{kj}  \\delta^{l+1}_k \\sigma'(z^l_j)\\\\\n",
    "\\end{align}\n",
    "\n",
    "Now we can return to the interpretation of equation (BP2). Observing the order of summation, namely that we are summing over the second index -- over the neurons of layer $l+1$ -- we are in effect first propagating the error at each of the (inputs to the) neurons of layer $l+1$ *back* through the linear combiner for layer $l$, and then through the activation function before arriving at the error to the input to the $j$th neuron in layer $l$. \n",
    "\n",
    "### (BP3) Rate of change of cost with respect to biases\n",
    "\n",
    "An application of the chain rule reveals that the rate of change of the cost function with respect to any bias is simply the error $\\delta^l_j$. \n",
    "\n",
    "### (BP4) Rate of change of cost with respect to network weights\n",
    "\n",
    "This equation can be written more heuristically as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial w} &= a_{in} \\delta_{out}\\\\\n",
    "\\\\\n",
    "\\end{align}\n",
    "\n",
    "We interpret $a_{in}$ as an input activation prior to the application of the weight; and $\\delta_{out}$ as the error that results from applying the weight. \n",
    "\n",
    "Note that if the activation $a_{in}$ is small, then the cost function varies slightly and therefore the network at this weight learns slowly. In other words, weights following low activation neurons learn slowly. \n",
    "\n",
    "### Other observations\n",
    "\n",
    "From (BP1), we observe that if the sigmoid function saturates then $\\sigma'(z^L) \\approx 0$ and the weight in the final layer learns slowly. Similar insights apply to earlier layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm\n",
    "\n",
    "Using the backpropagation equations (BP1) - (BP4) and a feedforward step we can formulate the algorithm which estimates the optimal parameters. \n",
    "\n",
    "**1. Given a set of training instances as input to the algorithm**\n",
    "\n",
    "**2. For each training example:** Start by assigning the inputs $x$ to $a^{x,1}$, and then proceed through the following steps:\n",
    "\n",
    "* **Feedforward**: For each of $l = 2, 3, ..., L$ calculate the affine transformation $z^{x,l} = w^l a^{x,l-1} + b^l$, and also $a^{x,l} = \\sigma(z^{x,l})$\n",
    "\n",
    "\n",
    "* **Output Error** $\\delta^{x,L}$  : Compute the error vector $\\delta^{x,L} = \\nabla_a C_x \\odot \\sigma'(z^{x,L})$\n",
    "\n",
    "\n",
    "* **Backpropagate Error**: Work back from $l = L-1, L-2, ...., 2$, computing the errors $\\delta\\,^{x,l} =  ((w^{l+1})^T \\delta\\,^{x,l+1} ) \\odot \\sigma'(z^{x,l})$\n",
    "\n",
    "**3. Perform gradient descent**: For each of $l = L, L-1, L-2, ...., 2$ update the network weights and biases according to the rules:\n",
    "\n",
    "\\begin{align}\n",
    "(SG1) \\qquad w^l \\leftarrow w'^l &= w^l - \\frac{\\eta}{m} \\sum_x \\delta^{x,l} (a^{x,l-1})^T \\\\ \\\\\n",
    "(SG2) \\qquad b^l \\leftarrow b'^l &= b^l - \\frac{\\eta}{m} \\sum_x \\delta^{x,l}  \\\\ & \n",
    "\\end{align}\n",
    "\n",
    "Here we are estimating the gradient of the cost function averaged over *all* the training samples by computing and averaging the gradient over $m$ randomly selected *mini-batch* of samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        # perform the feedforward in a recursive manner\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        \n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            \n",
    "            \"\"\"\n",
    "            Each mini batch contains a set of mini_batch_size (randomly selected) training instances.\n",
    "            There will be n / mini_batch_size such batches\n",
    "            \"\"\"\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print (\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print (\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \"\"\"\n",
    "        The gradient descent equations (SG1) and (SG2) require an estimate of the gradient \n",
    "        averaged over all the training instances. The estimate involves performing a summation\n",
    "        over the mini_batch_size samples in mini_batch.\n",
    "        \"\"\"\n",
    "        for x, y in mini_batch:\n",
    "            # Obtain the gradients with respect to the weights and biases via backpropagation\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            # Perform recursive summation\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        # Obtain weight and bias updates using equations (SG1) and (SG2)\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            # Construct the z-vectors -- linear combinations of the activations + biases\n",
    "            # defined in the equations above as z = W * a + b\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        # calculate errors for layer L (item -1 in activations, nabla_b and nabla_w)\n",
    "        # This is applying (BP1) from the backpropagation equations, and then (BP3) and (BP4)\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up network \n",
    "\n",
    "Three layers, so only one hidden layer containing 30 neurons. Since each sample consists of 28 X 28 = 784 pixels, the input layer has 784 neurons. There are 10 digits 0--9, so the output layer will have 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine network properties\n",
    "\n",
    "The number of layers of network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biases of the second and third layers are vectors of length 30 and 10 respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.biases[0].size, net.biases[1].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights for the second and third layers will be matrices of dimension 30 X 784 and 10 X 30 respectively. Note the order of rows and coloumns: because the weight matrix is a linear map from the activations of layer $l$ to layer $l+1$, the number of rows must equal the dimension of the image space, namely the number of neurons of the current layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 784), (10, 30))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weights[0].shape, net.weights[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "f = gzip.open('/Users/Rhizome73/src/snake-charmer/notebooks/neural-networks-and-deep-learning/data/mnist_py3k.pkl.gz', 'rb')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir, data_file = os.path.split(\"mnist_py3k.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('/Users/Rhizome73/src/snake-charmer/notebooks/neural-networks-and-deep-learning/data/mnist_py3k.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_d, va_d, te_d = load_data()\n",
    "training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "training_results = [vectorized_result(y) for y in tr_d[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = list(training_data)\n",
    "validation_data = list(validation_data)\n",
    "test_data = list(test_data)\n",
    "len(training_data), len(validation_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[40000][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[40000][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these data sets contains a tuple, where the first element of the tuple is a 784 length vector of the pixel intensities and the second element is the one-hot encoded output vector of length 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code deconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(training_data)\n",
    "random.shuffle(training_data)\n",
    "mini_batch_size = 10\n",
    "mini_batches = [\n",
    "    training_data[k:k+mini_batch_size]\n",
    "    for k in np.arange(0, n, mini_batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batches[0][1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batches[0][0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that each mini-batch contains a set of training instances of length `mini_batch_size` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
